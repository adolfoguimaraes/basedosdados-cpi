{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento das Entidades \n",
    "\n",
    "Nesse notebook, os arquivos gerados pelo BERT são processados para a geração dos arquivos finais com as entidades. O processamento consiste em eliminar algumas entidades que não ajudam muito no entendimento do texto, como: 'Presidente', 'Ministro', 'Sr', 'Sra'. O objetivo é deixar em evidência entidades que tragam mais informação para a análise dos textos processados, como por exemplo: Presidente da República, Senador Randolfe e assim por diante. \n",
    "\n",
    "Essa eliminação foi feita de forma manual, selecionando algumas entidades para cada uma das classes extraídas. \n",
    "\n",
    "Outro processamento realizado foi a unificação de termos como COVID e COVID-19. \n",
    "\n",
    "Por fim, as entidades das classes `COISA`, `ABSTRACCAO`, `OUTRO`, `OBRA` e `ACONTECIMENTO` foram agrupadas em uma única entidade: `OUTROS`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palavras removidas por entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = {'PESSOA': [\n",
    "    'Sr. Presidente',\n",
    "    'Sr. Relator',\n",
    "    'Prefeitos',\n",
    "    'Presidente',\n",
    "    'Senador',\n",
    "    'Senadora',\n",
    "    'Ministro',\n",
    "    'Governador',\n",
    "    'Senhor',\n",
    "    'Excelência',\n",
    "    'Relator',\n",
    "    'V. Exa.',\n",
    "    'V. Exa',\n",
    "    'V. Exas',\n",
    "    'Governador do',\n",
    "    'Srs.',\n",
    "    'Sr.',\n",
    "    'Sra',\n",
    "    'Sras',\n",
    "    'Sra.',\n",
    "    'Sras.',\n",
    "    'Sr',\n",
    "    'Srs',\n",
    "    'Secretário de',\n",
    "    'Covid-19',\n",
    "    'Governador',\n",
    "    'Governadores',\n",
    "    'Prefeitos',\n",
    "    'Sras. Senadoras',\n",
    "    'Senadores', 'Senadoras', 'Sr. Senador', 'Srs. Senadores', 'Deputado', 'Secretário', 'Secretários',\n",
    "    'Parlamentar', 'Parlamentares', 'Deputados', ],\n",
    "    'LOCAL': ['Território', 'Brasil', 'País', 'Município', 'Municípios', 'Estado', 'Estados', 'Países', 'Covid-19'],\n",
    "    \"ORGANIZACAO\": ['Município', 'Governo', 'Municípios', 'Estado', 'Estados', 'Países', 'País', 'Brasil', 'Covid-19', 'China'],\n",
    "    \"OUTROS\": ['Covid-19', 'País', 'Programa', 'Medicina']\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapeamento para unificar algunas alguns termos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_words = {\n",
    "    'Covid': 'Covid-19',\n",
    "    'Programa Nacional de Imunizações': 'PNI'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes que foram agrupadas em Outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_outros = ['COISA','ABSTRACCAO','OUTRO','OBRA','ACONTECIMENTO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir acessa cada arquivo gerado pelo modelo de Reconhecimento de Entidades nomeadas e agrupa em duas variáveis: `all_entites` que tem todas as entidades independente do dia e `all_entities_date` que agrupa por data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "path_ = \"../output/ner_depoentes_convidados/ner_files/\"\n",
    "\n",
    "all_files = os.listdir(path_)\n",
    "\n",
    "all_entites_by_date = {}\n",
    "all_entites = {}\n",
    "\n",
    "\n",
    "for file_ in all_files:\n",
    "    file_split = file_.split(\"_\")\n",
    "    date_ = file_split[1]\n",
    "    name_ = ' '.join(file_split[2:]).split(\".\")[0]\n",
    "    \n",
    "    with open(path_ + file_) as datafile:\n",
    "        data_ = json.load(datafile)\n",
    "\n",
    "    final_dict = {}\n",
    "    for data in data_[0]['entities']:\n",
    "        text_ = data['text']\n",
    "        text_ = text_.strip()\n",
    "\n",
    "        if text_ in map_words.keys():\n",
    "            text_ = map_words[text_]\n",
    "\n",
    "        class_ = data['class']\n",
    "\n",
    "        if class_ in class_to_outros:\n",
    "            class_ = 'OUTROS'\n",
    "        \n",
    "        if class_ in remove_words.keys():\n",
    "            words_to_remove = remove_words[class_]\n",
    "        else:\n",
    "            words_to_remove = []\n",
    "\n",
    "        if text_ not in words_to_remove:\n",
    "\n",
    "\n",
    "\n",
    "            if class_ not in final_dict.keys():\n",
    "                final_dict[class_] = {text_: 1}\n",
    "            else: \n",
    "                if text_ not in final_dict[class_].keys():\n",
    "                    final_dict[class_][text_] = 1\n",
    "                else:\n",
    "                    final_dict[class_][text_] += 1\n",
    "\n",
    "            if class_ not in all_entites.keys():\n",
    "                all_entites[class_] = {text_: 1}\n",
    "            else:\n",
    "                if text_ not in all_entites[class_].keys():\n",
    "                    all_entites[class_][text_] = 1\n",
    "                else:\n",
    "                    all_entites[class_][text_] += 1\n",
    "\n",
    "            if date_ not in all_entites_by_date.keys():\n",
    "                all_entites_by_date[date_] = {}\n",
    "\n",
    "            if class_ not in all_entites_by_date[date_].keys():\n",
    "                all_entites_by_date[date_][class_] = {text_: 1}\n",
    "            else: \n",
    "                if text_ not in all_entites_by_date[date_][class_].keys():\n",
    "                    all_entites_by_date[date_][class_][text_] = 1\n",
    "                else:\n",
    "                    all_entites_by_date[date_][class_][text_] += 1\n",
    "\n",
    "\n",
    "    for class_ in final_dict.keys():\n",
    "        final_dict[class_] = dict(sorted(final_dict[class_].items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    for class_ in all_entites.keys():\n",
    "        all_entites[class_] = dict(sorted(all_entites[class_].items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    for date_ in all_entites_by_date.keys():\n",
    "        for class_ in all_entites_by_date[date_].keys():\n",
    "            all_entites_by_date[date_][class_] = dict(sorted(all_entites_by_date[date_][class_].items(), key=lambda item: item[1], reverse=True))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dicionários criados são salvos em dois arquivos na pasta `output`: \n",
    "\n",
    "* `all_entites`: foi salvo em `todas_entidades_nomeadas.json`.\n",
    "* `all_entites_by_date`: foi salvo em `todas_entidades_nomeadas_por_data.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/todas_entidades_nomeadas.json\", 'w') as outfile:\n",
    "    json.dump(all_entites, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/todas_entidades_nomeadas_por_data.json\", 'w') as outfile:\n",
    "    json.dump(all_entites_by_date, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f872a193c411d785a417cb451ad65b981e6d66581d470666f03d0d69e08a512e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
