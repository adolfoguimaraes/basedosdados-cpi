{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo termos mais frequentes\n",
    "\n",
    "Este notebook processa dados textuais da CPI da Pandemia e extrai os termos mais relevantes de toda a base e os termos mais relevantes de cada sessão. Para isso, foi utilizado o conceito de TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Necessários\n",
    "\n",
    "import nltk \n",
    "import pandas as pd\n",
    "import json \n",
    "from nltk import ngrams\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords em Português\n",
    "stopwords_ptbr = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Os Dados\n",
    "\n",
    "Os dados foram extraídos da base de transcrições das sessões da pandemia que estão disponíveis no site da Base dos Dados: https://basedosdados.org/dataset/br-senado-cpipandemia. \n",
    "\n",
    "A seguinte consulta SQL foi utilizada: \n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  data_sessao, \n",
    "  SUM(duracao_discurso) duracao_discursos, \n",
    "  STRING_AGG(texto_discurso, ' ') as discurso \n",
    "FROM \n",
    "  `basedosdados.br_senado_cpipandemia.discursos`\n",
    "GROUP BY data_sessao\n",
    "ORDER BY data_sessao\n",
    "``` \n",
    "\n",
    "O resultado foi salvo no arquivo `input/discursos_por_dia.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_sessao</th>\n",
       "      <th>duracao_discursos</th>\n",
       "      <th>discurso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>15120</td>\n",
       "      <td>Invocando a proteção de Deus, declaro aberta a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-29</td>\n",
       "      <td>7920</td>\n",
       "      <td>Havendo número regimental, declaro aberta a 2ª...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-04</td>\n",
       "      <td>29040</td>\n",
       "      <td>Bom dia. Havendo número regimental, declaro ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-05</td>\n",
       "      <td>22560</td>\n",
       "      <td>Bom dia. Havendo número regimental, declaro ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>37320</td>\n",
       "      <td>Bom dia! Havendo número regimental, declaro ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_sessao  duracao_discursos  \\\n",
       "0  2021-04-27              15120   \n",
       "1  2021-04-29               7920   \n",
       "2  2021-05-04              29040   \n",
       "3  2021-05-05              22560   \n",
       "4  2021-05-06              37320   \n",
       "\n",
       "                                            discurso  \n",
       "0  Invocando a proteção de Deus, declaro aberta a...  \n",
       "1  Havendo número regimental, declaro aberta a 2ª...  \n",
       "2  Bom dia. Havendo número regimental, declaro ab...  \n",
       "3  Bom dia. Havendo número regimental, declaro ab...  \n",
       "4  Bom dia! Havendo número regimental, declaro ab...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"../input/discursos_por_dia.csv\")\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados foram tokenizados utilizando o `word_tokenize` do pacote NLTK mantendo apenas palavras e retirando as pontuações. Para cada seção é então extraído os termos mais frequentes que são ordenados do mais frequente para o menos frequente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (1/38) ... 2021-04-27\n",
      "Processing (2/38) ... 2021-04-29\n",
      "Processing (3/38) ... 2021-05-04\n",
      "Processing (4/38) ... 2021-05-05\n",
      "Processing (5/38) ... 2021-05-06\n",
      "Processing (6/38) ... 2021-05-11\n",
      "Processing (7/38) ... 2021-05-12\n",
      "Processing (8/38) ... 2021-05-13\n",
      "Processing (9/38) ... 2021-05-18\n",
      "Processing (10/38) ... 2021-05-19\n",
      "Processing (11/38) ... 2021-05-20\n",
      "Processing (12/38) ... 2021-05-25\n",
      "Processing (13/38) ... 2021-05-26\n",
      "Processing (14/38) ... 2021-05-27\n",
      "Processing (15/38) ... 2021-06-01\n",
      "Processing (16/38) ... 2021-06-02\n",
      "Processing (17/38) ... 2021-06-08\n",
      "Processing (18/38) ... 2021-06-09\n",
      "Processing (19/38) ... 2021-06-10\n",
      "Processing (20/38) ... 2021-06-11\n",
      "Processing (21/38) ... 2021-06-15\n",
      "Processing (22/38) ... 2021-06-16\n",
      "Processing (23/38) ... 2021-06-17\n",
      "Processing (24/38) ... 2021-06-18\n",
      "Processing (25/38) ... 2021-06-22\n",
      "Processing (26/38) ... 2021-06-23\n",
      "Processing (27/38) ... 2021-06-24\n",
      "Processing (28/38) ... 2021-06-25\n",
      "Processing (29/38) ... 2021-06-29\n",
      "Processing (30/38) ... 2021-06-30\n",
      "Processing (31/38) ... 2021-07-01\n",
      "Processing (32/38) ... 2021-07-06\n",
      "Processing (33/38) ... 2021-07-07\n",
      "Processing (34/38) ... 2021-07-08\n",
      "Processing (35/38) ... 2021-07-09\n",
      "Processing (36/38) ... 2021-07-13\n",
      "Processing (37/38) ... 2021-07-14\n",
      "Processing (38/38) ... 2021-07-15\n"
     ]
    }
   ],
   "source": [
    "corpus = {}\n",
    "vocabulary = {}\n",
    "for index, row in all_data.iterrows():\n",
    "    print(\"Processing (%i/%i) ... %s\" % (index + 1, all_data.shape[0], row['data_sessao'])) \n",
    "    content = [word.lower() for word in nltk.word_tokenize(row['discurso']) if word.isalpha() and word not in stopwords_ptbr]\n",
    "    \n",
    "    freq = nltk.FreqDist(content)\n",
    "\n",
    "    corpus[row['data_sessao']] = dict(freq)\n",
    "\n",
    "for day in corpus:\n",
    "    corpus[day] = sorted(corpus[day].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um vocabulário das palavras de todos os textos é criado. Nele são armazenado: o total de documentos que o texto aparece, a freqência do termo na coleção e a lista de documentos na qual o termo aparece. Isso será utilizado para o cálculo do TF-IDF de cada termo/documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ... 2021-04-27\n",
      "Processing ... 2021-04-29\n",
      "Processing ... 2021-05-04\n",
      "Processing ... 2021-05-05\n",
      "Processing ... 2021-05-06\n",
      "Processing ... 2021-05-11\n",
      "Processing ... 2021-05-12\n",
      "Processing ... 2021-05-13\n",
      "Processing ... 2021-05-18\n",
      "Processing ... 2021-05-19\n",
      "Processing ... 2021-05-20\n",
      "Processing ... 2021-05-25\n",
      "Processing ... 2021-05-26\n",
      "Processing ... 2021-05-27\n",
      "Processing ... 2021-06-01\n",
      "Processing ... 2021-06-02\n",
      "Processing ... 2021-06-08\n",
      "Processing ... 2021-06-09\n",
      "Processing ... 2021-06-10\n",
      "Processing ... 2021-06-11\n",
      "Processing ... 2021-06-15\n",
      "Processing ... 2021-06-16\n",
      "Processing ... 2021-06-17\n",
      "Processing ... 2021-06-18\n",
      "Processing ... 2021-06-22\n",
      "Processing ... 2021-06-23\n",
      "Processing ... 2021-06-24\n",
      "Processing ... 2021-06-25\n",
      "Processing ... 2021-06-29\n",
      "Processing ... 2021-06-30\n",
      "Processing ... 2021-07-01\n",
      "Processing ... 2021-07-06\n",
      "Processing ... 2021-07-07\n",
      "Processing ... 2021-07-08\n",
      "Processing ... 2021-07-09\n",
      "Processing ... 2021-07-13\n",
      "Processing ... 2021-07-14\n",
      "Processing ... 2021-07-15\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "total_words = 0\n",
    "for day in corpus:\n",
    "    print(\"Processing ... %s\" % day) \n",
    "    for word, score in corpus[day]:\n",
    "        total_words += 1\n",
    "        if word not in vocabulary.keys():\n",
    "            vocabulary[word] = {'total': 1, 'documents': [day], 'freq': score}\n",
    "        else:\n",
    "            vocabulary[word]['total'] += 1\n",
    "            vocabulary[word]['freq'] += score \n",
    "            vocabulary[word]['documents'].append(day)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF para cada documento\n",
    "\n",
    "Para cada par termo/documento é calculado o TF-IDF. Foi utilizada a seguinte fórmula: \n",
    "\n",
    "$TFIDF(t, d) = TF * IDF$\n",
    "\n",
    "onde,\n",
    "\n",
    "$TF = \\frac{f_t}{|d|}$\n",
    "\n",
    "onde, $f$ é a frequência do termo no documento e $d$ o total de palavras no documento. \n",
    "\n",
    "$IDF = log_{10}(\\frac{N}{Total_{d,t}})$\n",
    "\n",
    "onde $N$ é o total de documentos da base (cada dia foi considerado um documento) e $Total_{d,t}$ é  o número de documentos que o termo aparece. \n",
    "\n",
    "O trecho de código a seguir faz o cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = {}\n",
    "short_corpus = {}\n",
    "total_documents = len(corpus.keys())\n",
    "acc_total_words = 0\n",
    "for day in corpus:\n",
    "    final_corpus[day] = {}\n",
    "    content = corpus[day]\n",
    "    total_words = len(set(content))\n",
    "    acc_total_words += total_words\n",
    "    dict_ = {\n",
    "        word[0]: {\n",
    "            'freq': word[1], \n",
    "            'tf-idf': (word[1] / total_words) * np.log10(total_documents / vocabulary[word[0]]['total']), \n",
    "            'documents': vocabulary[word[0]]['documents']\n",
    "            } for word in content}\n",
    "    final_corpus[day] = dict_\n",
    "    final_corpus[day] = sorted(final_corpus[day].items(), key=lambda x: x[1]['tf-idf'], reverse=True)\n",
    "    short_corpus[day] = dict(final_corpus[day][:250])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima foi utilizado para gerar os termos mais relevantes de cada dia de sessão da CPI. No entanto, eu queria extrair informação parecida de toda a coleção. Para isso usei o IDF de cada termo ponderado pela frequência dele em toda coleção. O código a seguir faz este cálculo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voc in vocabulary:\n",
    "    vocabulary[voc]['idf'] = (vocabulary[voc]['freq']) * (np.log10(len(corpus.keys()) / vocabulary[voc]['total']))\n",
    "\n",
    "vocab_sorted = sorted(vocabulary.items(), key=lambda x: x[1]['idf'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso foram gerados dois arquivos de saída:\n",
    "\n",
    "* `corpus_tfid.json`: com o TF-IDF de cada termo/documento. Observe que salvamos apenas os 250 termos mais relevantes de cada dia. Por isso, estamos salvando a variável `short_corpus`. \n",
    "* `vocabulary_full.json`: com o IDF ponderado pela frequência de toda a base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/corpus_tfid.json\", 'w') as outfile:\n",
    "    json.dump(short_corpus, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/vocabulary_full.json\", 'w') as outfile:\n",
    "    json.dump(vocab_sorted, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "550301686af6c7722ac40de22451ca0d27c0924aed301874cfb5851c7eaa6b31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('basedosdados')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
